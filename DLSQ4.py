# -*- coding: utf-8 -*-
"""test_gpu.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/13FA3yrqDVE7dFveZlvXtGSZFUwWrFvx0
"""

import math
from sklearn.model_selection import train_test_split
import numpy as np
from keras import backend as K
from keras.layers import *
from keras.models import *
import torch

device = ("cuda" if torch.cuda.is_available() else "cpu")

def fx1x2(x1, x2):
  A = -(x2+47)*math.sin(math.sqrt(math.fabs((x1/2) + (x2+47))))
  B = -x1*math.sin(math.sqrt(math.fabs(x1-(x2+47))))
  return A + B

def make_dataset():
  samples = np.random.normal(0, 0.3, 100000)
  x1x2 = (np.random.rand(100000,2) - 0.5)*2*512

  f = []

  for i in range(len(x1x2)):
    temp = fx1x2(x1x2[i][0], x1x2[i][1])
    f.append(temp)

  f = np.array(f)

  y = f + samples

  x_train, x_test, y_train, y_test = train_test_split(x1x2, y, test_size=0.2)

  x_train = x_train.reshape(80000, 2, 1)
  x_test = x_test.reshape(20000, 2, 1)

  return x_train, x_test, y_train, y_test

def root_mean_squared_error(y_true, y_pred):
  return K.sqrt(K.mean(K.square(y_pred - y_true), axis=-1))

def make_model(x_train, y_train):
  model = Sequential()
  model.add(Dense(units=32, kernel_initializer='normal', activation='relu'))
  model.add(BatchNormalization())
  model.add(Dense(units=1, kernel_initializer='normal', activation='softmax'))
  model.compile(optimizer='adam', loss='mean_squared_error')
  save_history = model.fit(x_train, y_train, epochs=10, batch_size=64, validation_split=0.2)
  return save_history

if __name__ == "__main__":
  x_train, x_test, y_train, y_test = make_dataset()
  save_history = make_model(x_train, y_train)





